<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Neural Point Cloud Rendering via Multi-Plane Projection</title>
  <link href="css/style.css" rel="stylesheet" type="text/css" />

  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
<body>
  <div class="container"> 
    <span class="title">Neural Point Cloud Rendering via Multi-Plane Projection</span>
    <table border="0" align="center" class="authors">
      <tr align="center">
        <td><a href="">Peng Dai*</a></td>
        <td><a href="https://www.zhangyinda.com/">Yinda Zhang*</a></td>
        <td><a href="https://scholar.google.com/citations?user=gIBLutQAAAAJ&hl=en">Zhuwen Li*</a></td>
        <td> <a href="http://www.liushuaicheng.org/">Shuaicheng Liu</a> </td>
        <td> <a href="https://scholar.google.com/citations?user=s-kUGYQAAAAJ&hl=en">Bing Zeng</a> </td>
      </tr>
    </table>
    <table border="0" align="center" class="affiliations">
      <tr>
        <td align="center"> University of Electronic Science and Technology of China</td>
      </tr>
      <tr>
        <td align="center"> Google Research</td>
      </tr>
      <tr>
        <td align="center"> Nuro.Inc</td>
      </tr>  
    </table>

    <p>&nbsp;</p>
    <table width="200" border="0" align="center">
      <tr>
        <td><img src="images/teaser.png" width="700" alt="" /><br /></td>
      </tr>
      <tr>
        <td class="caption">Our proposed method synthesize images in novel view by using neural point cloud rendering.  </td>
      </tr>
    </table>

    <p>&nbsp;</p>
    <span class="section">Abstract</span>
    <p>We present a new deep point cloud rendering pipeline
      through multi-plane projections. The input to the network is
      the raw point cloud of a scene and the output are image or
      image sequences from a novel view or along a novel camera
      trajectory. Unlike previous approaches that directly project
      features from 3D points onto 2D image domain, we propose
      to project these features into a layered volume of camera
      frustum. In this way, the visibility of 3D points can be automatically
      learnt by the network, such that ghosting effects
      due to false visibility check as well as occlusions caused by
      noise interferences are both avoided successfully. Next, the
      3D feature volume is fed into a 3D CNN to produce multiple
      planes of images w.r.t. the space division in the depth
      directions. The multi-plane images are then blended based
      on learned weights to produce the final rendering results.
      Experiments show that our network produces more stable
      renderings compared to previous methods, especially near
      the object boundaries. Moreover, our pipeline is robust to
      noisy and relatively sparse point cloud for a variety of challenging
      scenes.<br />
    </p>

    <p class="section">&nbsp;</p>
    <span class="section">Framework</span>
    <table width="200" border="0" align="center">
      <tbody>
        <tr>
          <p></p>
          <td align="center"><img src="images/architecture.png" height="280" alt="" /></td>
        </tr>
      </tbody>
    </table>
    <p>
      <td class="caption">Our method is divided into two parts, the multi-plane based voxelization (left) and multi-plane
        rendering(right). For the first part, point clouds are re-projected into camera coordinate system to form frustum region and voxelization
        plus aggregation operations are adopted to generate a multi-plane 3D representation, which will be concatenated with normalized view
        direction and sent to render network. For the second part, the concatenated input is feed into a 3D neural render network to predict the
        product with 4 channels (i.e. RGB + blend weight) and the final output is generated by blending all planes. The training process is under
        the supervision of perceptual loss, and both network parameters and point clouds features are optimized according to the gradient.</td>
    </p>

    <p>&nbsp;</p>
    <p class="section">Video</p>
    <object width="720" height="576" data="https://www.youtube.com/watch?v=nkiP5QnIOo8"></object>
    

    <p>&nbsp;</p>
    <p class="section">Paper</p>
    <table border="0">
      <tbody>
        <tr>
          <td><img src="images/paper.png" width="100" alt="" /></td>
          <td>&nbsp;</td>
          <td>
            <p>&quot;Neural Point Cloud Rendering via Multi-Plane Projection&quot;,<br/>
              Peng Dai*, Yinda Zhang*, Zhuwen Li*, Shuaicheng Liu, Bing Zeng<br/>
              Conference on Computer Vision and Pattern Recongnition (CVPR), 2020</p>
            <p>[<a href="https://arxiv.org/abs/1912.04645">PDF</a>]
              [<a href="">Code coming soon</a>]
              [<a href="">Slides</a>][<a href="https://drive.google.com/open?id=1-_ZibCEyVGF80hleQOfKmqDR-ewpdQbc" >supplimentary video</a>] </p>
          </td>
        </tr>
      </tbody>
    </table>

    <p class="section">&nbsp;</p>
    <p align="center" class="date">Last updated: April 2020</p>
  </div>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$']]}
  });
  </script>
</body>

</html>
