<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <title>Neural Point Cloud Rendering via Multi-Plane Projection</title>
  <link href="css/style.css" rel="stylesheet" type="text/css" />

  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
<body>
  <div class="container"> 
    <span class="title">Neural Point Cloud Rendering via Multi-Plane Projection</span>
    <table border="0" align="center" class="authors">
      <tr align="center">
        <td><a href="">Peng Dai*</a></td>
        <td><a href="https://www.zhangyinda.com/">Yinda Zhang*</a></td>
        <td><a href="https://scholar.google.com/citations?user=gIBLutQAAAAJ&hl=en">Zhuwen Li*</a></td>
        <td> <a href="http://www.liushuaicheng.org/">Shuaicheng Liu</a> </td>
        <td> <a href="https://scholar.google.com/citations?user=s-kUGYQAAAAJ&hl=en">Bing Zeng</a> </td>
      </tr>
    </table>
    <table border="0" align="center" class="affiliations">
      <tr>
        <td align="center"> University of Electronic Science and Technology of China</td>
      </tr>
      <tr>
        <td align="center"> Google Research</td>
      </tr>
      <tr>
        <td align="center"> Nuro.Inc</td>
      </tr>  
    </table>

    <p>&nbsp;</p>
    <table width="200" border="0" align="center">
      <tr>
        <td><img src="images/teaser.png" width="700" alt="" /><br /></td>
      </tr>
      <tr>
        <td class="caption">Our proposed method synthesize images in novel view by using multi-plane neural point cloud rendering.  </td>
      </tr>
    </table>

    <p>&nbsp;</p>
    <span class="section">Abstract</span>
    <p>We present a new deep point cloud rendering pipeline
      through multi-plane projections. The input to the network is
      the raw point cloud of a scene and the output are image or
      image sequences from a novel view or along a novel camera
      trajectory. Unlike previous approaches that directly project
      features from 3D points onto 2D image domain, we propose
      to project these features into a layered volume of camera
      frustum. In this way, the visibility of 3D points can be automatically
      learnt by the network, such that ghosting effects
      due to false visibility check as well as occlusions caused by
      noise interferences are both avoided successfully. Next, the
      3D feature volume is fed into a 3D CNN to produce multiple
      planes of images w.r.t. the space division in the depth
      directions. The multi-plane images are then blended based
      on learned weights to produce the final rendering results.
      Experiments show that our network produces more stable
      renderings compared to previous methods, especially near
      the object boundaries. Moreover, our pipeline is robust to
      noisy and relatively sparse point cloud for a variety of challenging
      scenes.<br />
    </p>
    
    <p>&nbsp;</p>
    <p class="section">Documents</p>
    <table border="0">
      <tbody>
        <tr>
          <td><img src="images/paper.png" width="100" alt="" /></td>
          <td>&nbsp;</td>
          <td>
            <p>&quot;Neural Point Cloud Rendering via Multi-Plane Projection&quot;,<br/>
              Peng Dai*, Yinda Zhang*, Zhuwen Li*, Shuaicheng Liu, Bing Zeng<br/>
              Conference on Computer Vision and Pattern Recongnition (CVPR), 2020</p>
            <p>[<a href="https://arxiv.org/abs/1912.04645">PDF</a>]
              [<a href="https://github.com/daipengwa/NeuralPointCloudRendering">Code</a>]
              [<a href="https://drive.google.com/open?id=1ajlhVKewXAXToNnW0Qkupzo2igVwQj99">Slides</a>]
              [<a href="https://drive.google.com/open?id=1V4DNSMfBF2LQ1ZIcaGo0auLgwpcbaP83">Poster</a>]
              [<a href="https://drive.google.com/open?id=1-_ZibCEyVGF80hleQOfKmqDR-ewpdQbc" >supplimentary video</a>] </p>
          </td>
        </tr>
      </tbody>
    </table>
    
    <p>&nbsp;</p>
    <p class="section">Motivations</p>
    <p> Recently, neural point cloud rendering has obtained increasing attention. In order to obtain statisfying rendering results, 
      researchers have proposed various techniques, such as neural texture. However, there are weaknesses existing in previous methods, 
      which don't explicitly take point cloud issues into consideration. For example, duo to imprecise depth and estimated camera parameters,
      point clouds are usually noisy, escipically at objects' boundary. Also, sparse points will result in confusion between foregroud and background. 
      What's more, rasterizing point clouds via z-buffer into the 2D plane tends to cause flickers and non-uniform optimization. 
    </p>

    <p class="section">&nbsp;</p>
    <span class="section">Framework</span>
    <table width="200" border="0" align="center">
      <tbody>
        <tr>
          <p></p>
          <td align="center"><img src="images/architecture.png" height="280" alt="" /></td>
        </tr>
      </tbody>
    </table>
    <p>
      <td class="caption">Our method is divided into two parts, the multi-plane based voxelization (left) and multi-plane
        rendering(right). For the first part, point clouds are re-projected into camera coordinate system to form frustum region and voxelization
        plus aggregation operations are adopted to generate a multi-plane 3D representation, which will be concatenated with normalized view
        direction and sent to render network. For the second part, the concatenated input is feed into a 3D neural render network to predict the
        product with 4 channels (i.e. RGB + blend weight) and the final output is generated by blending all planes. The training process is under
        the supervision of perceptual loss, and both network parameters and point clouds features are optimized according to the gradient.</td>
    </p>

    <p>&nbsp;</p>
    <p class="section">Results</p>
    <table width="200" border="0" align="center">
      <tr>
        <td><img src="images/occulusion.png" width="700" alt="" /><br /></td>
      </tr>
      <tr>
        <td class="caption"> Instead of discarding occluded points via z-buffer rasterization, we proposed to keep them in different depth-related planes, 
          Which is proved to effectively aviod artifacts caused by false visibility check. </td>
      </tr>
    </table>
        <table width="200" border="0" align="center">
      <tr>
        <td><img src="images/sparsity.png" width="700" alt="" /><br /></td>
      </tr>
      <tr>
        <td class="caption"> Due to the sparsity of points, foreground and background informaiton are entangled. 
          Our multi-plane rendering pipeline can appropriately handle it under relatively sparse points. </td>
      </tr>
    </table>
    

    
    <p>&nbsp;</p>
    <p class="section">Video</p>
      <div style="text-align:center;">
        <iframe width="926" height="525" src="https://www.youtube.com/embed/iWehgsCjZZE" frameborder="0" allow="accelerometer; autoplay; picture-in-picture" allowfullscreen></iframe>
      </div>
    

    <p class="section">&nbsp;</p>
    <p align="center" class="date">Last updated: July 2020</p>
  </div>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$']]}
  });
  </script>
</body>

</html>
